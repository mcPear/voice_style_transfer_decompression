Noatki AutoVC

AUTOVC works on the speech mel-spectrogram of size N-by-T, where N is the number of mel-frequency bins and T is
the number of time steps (frames).
As shown in Fig. 3(a), the input to the content encoder is the
80-dimensional mel-spectrogram of X1 concatenated with
the speaker embedding, Es(X1), at each time step. 

In our implementation, the frame rate of the
mel-spetrogram is 62.5 Hz and the sampling rate of speech
waveform is 16 kHz.

ZADANIA:
1. Wypowiedź -> mel - jaka wypowiedź, jakie parametry mel ?
2. mel -> style mebedding - TD czy TI ? jaki rozmiar, jak ?
3. Transformacja embeddingu i mel tak, żeby odpowiadało wejściu AutoVC i odpalenie

IMPROVEMENTS:
1. train style encoder on all utterances(without train/test split)
1. train style encoder in text dependent version (handles noise propably but do we need it?)

enroll_embed - mean of utterances(centroid), mean of verif_embed during test
verif_embed - all utterances of speaker and (what is important) also splitted to non silent parts
centroid embedding should be same as separate embeddings!

autovc params:
embedding - 256
mel size - 80

E2EGL default params:
embedding - 256 - but implementation has bug, should set it for tisv
mel size - 40
------
Dekompresja
Czy inny rozmiar wypowiedzi zmieni zawartość informacji o treści/stylu w embeddingu ?
Czy to będzie ten sam sposób uczenia i loss?
Można zachować już wyuczone wagi i od nich startować, albo uczyć tylko warstwę embeddingu, a resztę usztywnić.

Podczas ładowania modelu trzeba sprawdzić jaki model_num daje najmniejszy błąd.

!!@@!!Praca w dockerze gpu wymaga restartowania kernela po modyfikacji pliku i podawania wewnętrznych ścieżek kontenera

Model wyuczył się na float32(DT_FLOAT), a powinien na float16(DT_HALF), bo byłoby szybciej. Nie wiem dlaczego podczas testu graf był float16, a podczas treningu uczył się float32...
Jeśli będzie wolny feedforward na float32 to można jeszcze raz wyuczyć na float16, ale prawdopodobnie nie będzie trzeba. Dodałem funkcję printującą graf z typami zmiennych.

Batche:
enrollment - średnia z kilku
verif - pojedynczy ostatni
ale ten podział jest tylko w test
w train jest jeden batch

Split działa tak, ze dzieli na części gdzie jest głos (>20db) i wybiera pierwsze i ostatnie 180 ramek z każdej części.
Ciekawe czemu nie wszystkie ?

Jest problem z długością spektrogramu. Nie wiem czy to zależy od sampling rate, ale w autoVC są dużo krótze te spektrogramy. Trzeba wziąć wypowiedź i spektogram i rozkminić jak oni zamieniają, a potem może będzie trzeba wyuczyć na takich parametrach...

------------Voice Conversion, Chou et al.-------------
W kroku pierwszym uczymy się kodować treść ucząc autokodera i minimalizujac prawdopodobieństwo, że wypowiedź należy do tego mówcy, żeby uniezależnić embedding od mówcy. Ale nie wiem jak jest liczone to prawdopodobieństwo.
Można zaatakować na większym zbiorze.
