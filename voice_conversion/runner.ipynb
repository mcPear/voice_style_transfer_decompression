{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 1.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.9.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from tensorboardX) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (41.2.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/95/90e8c4c31cfc67248bf944ba42029295b77159982f532c5689bcfe4e9108/torch-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (734.6MB)\n",
      "\u001b[K     |████████████████████████████████| 734.6MB 125kB/s  eta 0:00:01     |███████████▋                    | 266.5MB 12.2MB/s eta 0:00:39     |███████████████▉                | 364.0MB 10.5MB/s eta 0:00:36     |████████████████████████████▍   | 652.5MB 12.1MB/s eta 0:00:07\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.2)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.3.1\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting librosa\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/71/c21ccef81276d85b9e3a36d80dad5baaf8a91f912e65eff0fd0d74a5a19c/librosa-0.7.1.tar.gz (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 1.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting audioread>=2.0.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/2e/0b/940ea7861e0e9049f09dcfd72a90c9ae55f697c17c299a323f0148f913d2/audioread-2.1.8.tar.gz\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.17.2)\n",
      "Collecting scipy>=1.0.0 (from librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1MB 12.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn!=0.19.0,>=0.14.0 (from librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/48/e9fa9e252abcd1447eff6f9257636af31758a6e46fd5ce5d3c879f6907cb/scikit_learn-0.22.1-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 11.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.12 (from librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: six>=1.3 in /usr/lib/python3/dist-packages (from librosa) (1.11.0)\n",
      "Collecting resampy>=0.2.2 (from librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/75/e22272b9c2185fc8f3af6ce37229708b45e8b855fd4bc38b4d6b040fff65/resampy-0.2.2.tar.gz (323kB)\n",
      "\u001b[K     |████████████████████████████████| 327kB 14.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.43.0 (from librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/7d/f2b5ea8d5952115351e303d1aadbdd1c5b08f479d76456d43a5d7a3a8c88/numba-0.47.0-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 7.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soundfile>=0.9.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa) (41.2.0)\n",
      "Collecting llvmlite>=0.31.0dev0 (from numba>=0.43.0->librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/bb/60d4033d56c9da36490af19caa6c794b72b8aef6f792fdfa8cb95d11e419/llvmlite-0.31.0-cp36-cp36m-manylinux1_x86_64.whl (20.2MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2MB 10.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cffi>=1.0 (from soundfile>=0.9.0->librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/72/0d42f94fe94afa8030350c26e9d787219f3f008ec9bf6b86c66532b29236/cffi-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (397kB)\n",
      "\u001b[K     |████████████████████████████████| 399kB 8.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycparser (from cffi>=1.0->soundfile>=0.9.0->librosa)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 14.0MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: librosa, audioread, resampy, pycparser\n",
      "  Building wheel for librosa (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for librosa: filename=librosa-0.7.1-cp36-none-any.whl size=1611357 sha256=f6067fc25e0a8ca3349a902ff71846249f920d824eb8138984c45a0ff21f0546\n",
      "  Stored in directory: /root/.cache/pip/wheels/07/36/47/a9a4d151332cbdaec564500af9704a0ad862cf554dcf4bfda0\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-2.1.8-cp36-none-any.whl size=26091 sha256=9a3de1d792612e22016d137a451f3b5b599418b6cb3d487347019c703fc2ad64\n",
      "  Stored in directory: /root/.cache/pip/wheels/b9/64/09/0b6417df9d8ba8bc61a7d2553c5cebd714ec169644c88fc012\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-cp36-none-any.whl size=321053 sha256=108ab420808d8edb4ba6213652147266be7bad711190b495963649a3d456001d\n",
      "  Stored in directory: /root/.cache/pip/wheels/fa/c1/56/e0e12c6f7f3d2cdea9712b35136a2d40a7817c6210ec096485\n",
      "  Building wheel for pycparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=112040 sha256=a61a2dfea6572ba6dbb6c5278537ac13d77fe96ed414539ce0bcddddd2a487f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
      "Successfully built librosa audioread resampy pycparser\n",
      "Installing collected packages: audioread, scipy, joblib, scikit-learn, llvmlite, numba, resampy, pycparser, cffi, soundfile, librosa\n",
      "Successfully installed audioread-2.1.8 cffi-1.13.2 joblib-0.14.1 librosa-0.7.1 llvmlite-0.31.0 numba-0.47.0 pycparser-2.19 resampy-0.2.2 scikit-learn-0.22.1 scipy-1.4.1 soundfile-0.10.3.post1\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libflac8 libogg0 libvorbis0a libvorbisenc2\n",
      "The following NEW packages will be installed:\n",
      "  libflac8 libogg0 libsndfile1 libvorbis0a libvorbisenc2\n",
      "0 upgraded, 5 newly installed, 0 to remove and 27 not upgraded.\n",
      "Need to get 557 kB of archives.\n",
      "After this operation, 2051 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libogg0 amd64 1.3.2-1 [17.2 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libflac8 amd64 1.3.2-1 [213 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libvorbis0a amd64 1.3.5-4.2 [86.4 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libvorbisenc2 amd64 1.3.5-4.2 [70.7 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsndfile1 amd64 1.0.28-4ubuntu0.18.04.1 [170 kB]\n",
      "Fetched 557 kB in 1s (1043 kB/s)    \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libogg0:amd64.\n",
      "(Reading database ... 15402 files and directories currently installed.)\n",
      "Preparing to unpack .../libogg0_1.3.2-1_amd64.deb ...\n",
      "Unpacking libogg0:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libflac8:amd64.\n",
      "Preparing to unpack .../libflac8_1.3.2-1_amd64.deb ...\n",
      "Unpacking libflac8:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libvorbis0a:amd64.\n",
      "Preparing to unpack .../libvorbis0a_1.3.5-4.2_amd64.deb ...\n",
      "Unpacking libvorbis0a:amd64 (1.3.5-4.2) ...\n",
      "Selecting previously unselected package libvorbisenc2:amd64.\n",
      "Preparing to unpack .../libvorbisenc2_1.3.5-4.2_amd64.deb ...\n",
      "Unpacking libvorbisenc2:amd64 (1.3.5-4.2) ...\n",
      "Selecting previously unselected package libsndfile1:amd64.\n",
      "Preparing to unpack .../libsndfile1_1.0.28-4ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking libsndfile1:amd64 (1.0.28-4ubuntu0.18.04.1) ...\n",
      "Setting up libogg0:amd64 (1.3.2-1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Setting up libvorbis0a:amd64 (1.3.5-4.2) ...\n",
      "Setting up libflac8:amd64 (1.3.2-1) ...\n",
      "Setting up libvorbisenc2:amd64 (1.3.5-4.2) ...\n",
      "Setting up libsndfile1:amd64 (1.0.28-4ubuntu0.18.04.1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n"
     ]
    }
   ],
   "source": [
    "!python3.6 -m pip install tensorboardX\n",
    "!python3.6 -m pip install torch\n",
    "!python3.6 -m pip install librosa\n",
    "!apt-get install libsndfile1 --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from model_vctk/fixed_8bit_16k_paused_reusmed_latschediters_50k/model_vctk-80000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Encoder:\n\tsize mismatch for conv1s.0.weight: copying a param with shape torch.Size([128, 513, 1]) from checkpoint, the shape in current model is torch.Size([128, 80, 1]).\n\tsize mismatch for conv1s.1.weight: copying a param with shape torch.Size([128, 513, 2]) from checkpoint, the shape in current model is torch.Size([128, 80, 2]).\n\tsize mismatch for conv1s.2.weight: copying a param with shape torch.Size([128, 513, 3]) from checkpoint, the shape in current model is torch.Size([128, 80, 3]).\n\tsize mismatch for conv1s.3.weight: copying a param with shape torch.Size([128, 513, 4]) from checkpoint, the shape in current model is torch.Size([128, 80, 4]).\n\tsize mismatch for conv1s.4.weight: copying a param with shape torch.Size([128, 513, 5]) from checkpoint, the shape in current model is torch.Size([128, 80, 5]).\n\tsize mismatch for conv1s.5.weight: copying a param with shape torch.Size([128, 513, 6]) from checkpoint, the shape in current model is torch.Size([128, 80, 6]).\n\tsize mismatch for conv1s.6.weight: copying a param with shape torch.Size([128, 513, 7]) from checkpoint, the shape in current model is torch.Size([128, 80, 7]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([512, 1409, 1]) from checkpoint, the shape in current model is torch.Size([512, 976, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0554ce74d078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mhps_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavenet_mel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwavenet_mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavenet_mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mspec_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_source_8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavenet_mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/solver.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_path, enc_only)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mall_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Encoder:\n\tsize mismatch for conv1s.0.weight: copying a param with shape torch.Size([128, 513, 1]) from checkpoint, the shape in current model is torch.Size([128, 80, 1]).\n\tsize mismatch for conv1s.1.weight: copying a param with shape torch.Size([128, 513, 2]) from checkpoint, the shape in current model is torch.Size([128, 80, 2]).\n\tsize mismatch for conv1s.2.weight: copying a param with shape torch.Size([128, 513, 3]) from checkpoint, the shape in current model is torch.Size([128, 80, 3]).\n\tsize mismatch for conv1s.3.weight: copying a param with shape torch.Size([128, 513, 4]) from checkpoint, the shape in current model is torch.Size([128, 80, 4]).\n\tsize mismatch for conv1s.4.weight: copying a param with shape torch.Size([128, 513, 5]) from checkpoint, the shape in current model is torch.Size([128, 80, 5]).\n\tsize mismatch for conv1s.5.weight: copying a param with shape torch.Size([128, 513, 6]) from checkpoint, the shape in current model is torch.Size([128, 80, 6]).\n\tsize mismatch for conv1s.6.weight: copying a param with shape torch.Size([128, 513, 7]) from checkpoint, the shape in current model is torch.Size([128, 80, 7]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([512, 1409, 1]) from checkpoint, the shape in current model is torch.Size([512, 976, 1])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils import Hps\n",
    "from preprocess.tacotron.norm_utils import spectrogram2wav, get_spectrogram\n",
    "from scipy.io.wavfile import write\n",
    "import glob\n",
    "import os\n",
    "from solver import Solver\n",
    "import librosa\n",
    "import librosa.display\n",
    "from plots import plot\n",
    "%matplotlib inline\n",
    "\n",
    "args_hps='vctk.json'\n",
    "args_model=\"model_vctk/fixed_8bit_16k_paused_reusmed_latschediters_50k/model_vctk-80000\"\n",
    "args_source=\"test/p225_001.wav\"\n",
    "args_source_8=\"test/p225_001-8.wav\"\n",
    "args_target=0\n",
    "args_output=\"test/output.wav\"\n",
    "args_sample_rate=16000\n",
    "args_use_gen=False\n",
    "wavenet_mel = True\n",
    "\n",
    "hps = Hps()\n",
    "hps.load(args_hps)\n",
    "hps_tuple = hps.get_tuple()\n",
    "solver = Solver(hps_tuple, None, wavenet_mel=wavenet_mel)\n",
    "solver.load_model(args_model)\n",
    "spec = get_spectrogram(args_source, wavenet_mel)\n",
    "spec_8 = get_spectrogram(args_source_8, wavenet_mel)\n",
    "\n",
    "spec_expand = np.expand_dims(spec_8, axis=0)\n",
    "spec_tensor = torch.from_numpy(spec_expand).type(torch.FloatTensor)\n",
    "c = Variable(torch.from_numpy(np.array([int(args_target)]))).cuda()\n",
    "result = solver.test_step(spec_tensor, c, gen=args_use_gen)\n",
    "result = result.squeeze(axis=0).transpose((1, 0))\n",
    "wav_data = spectrogram2wav(result)\n",
    "write(args_output, rate=args_sample_rate, data=wav_data)\n",
    "\n",
    "plot(spec.T, spec_8.T, result.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train code copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a8b962fb179f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_load_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_output_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_flag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pretrain_G'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_output_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_flag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pretrain_D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_output_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_flag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, flag, mode)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pretrain_G'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_pretrain_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_trg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;31m# encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mbatch_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0msamples_trg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_trg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mbatch_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0msamples_trg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_trg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils import Hps\n",
    "from utils import DataLoader\n",
    "from utils import Logger\n",
    "from utils import SingleDataset\n",
    "from solver import Solver\n",
    "import argparse\n",
    "%matplotlib inline\n",
    "\n",
    "args_load_model=False\n",
    "args_flag='train'\n",
    "args_hps_path=\"/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/vctk.json\"\n",
    "args_dataset_path_trg=\"/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/vctk_old/data.h5\"\n",
    "args_dataset_path=\"/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/vctk_old/data-8-wavenet.h5\"\n",
    "args_index_path_trg=\"/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/vctk_old/index.json\"\n",
    "args_index_path=\"/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/vctk_old/index.json\"\n",
    "args_output_model_path=\"/tf/notebooks/SKAJPAI/voice_style_transfer/voice_conversion/model_vctk/model_vctk\"\n",
    "args_load_model_path=\"model_vctk/model_vctk-32000\"\n",
    "wavenet_mel=True\n",
    "\n",
    "hps = Hps()\n",
    "hps.load(args_hps_path)\n",
    "hps_tuple = hps.get_tuple()\n",
    "dataset = SingleDataset(args_dataset_path, args_index_path, seg_len=hps_tuple.seg_len)\n",
    "dataset_trg = SingleDataset(args_dataset_path_trg, args_index_path_trg, seg_len=hps_tuple.seg_len)\n",
    "data_loader = DataLoader(dataset, dataset_trg)\n",
    "\n",
    "solver = Solver(hps_tuple, data_loader, wavenet_mel)\n",
    "if args_load_model:\n",
    "    solver.load_model(args_load_model_path)\n",
    "\n",
    "solver.train(args_output_model_path, args_flag, mode='pretrain_G')\n",
    "solver.train(args_output_model_path, args_flag, mode='pretrain_D')\n",
    "solver.train(args_output_model_path, args_flag, mode='train')\n",
    "solver.train(args_output_model_path, args_flag, mode='patchGAN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
